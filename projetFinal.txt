#Projet du jour
#Amelioration de la pipeline
    -la rendre plus robuste ingestion des données 
    -Mettre en place du machine learning enfin d'en extraire le maximum d'informations
#Big data "authentique" avec Spark:
    -Remplacer Pandas par pySpark dans la pipeline ELT existante
    -Ajouter un cluster Spark (1 master et 2 workers via docker compose)
    -Convertir leur transformations Pandas en transformations Spark
    -Calculer le temps de traitement des 2 
#Base NoSQL opérationnelle avec MongoDB
    -pipeline qui lit gold (parquet) et ecrit dans MongoDB
    -API Flask /Flask API qui expose les données MongoDB dashboard Streamlit qui interroge l'API
    -calculer le temps de refresh entre les 2
    -Bonus: mise en place de la metabase pour faire un dashboard
#Streaming en temps réel avec Kafka+Spark Streaming
    -producteur kafka: simule des évenements e-commerce(achats, clicks, paniers abonnements)
    -spark streaming: consomme kafka, applique des transformations, detecte des anomalies

exo_python/
│
├── data/
│   ├── sources/
    │   ├── achats.csv
    │   └── clients.csv
│
├── ingestion/
│   └── ingest.py
│
├── spark_pipeline/
│   ├── transform.py
│   └── Dockerfile
│
├── mongodb/
│   ├── loader.py
│   └── Dockerfile
│
├── api/
│   ├── app.py
│   └── Dockerfile
│
├── dashboard/
│   ├── app.py
│   ├── _pycache_
│   └── config.py
│
├── flow/
│   ├── producer.py
│   └── consumer.py
│
├── docker-compose.yml
└── requirements.txt
